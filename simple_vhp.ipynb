{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f169896c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd.functional import vhp\n",
    "import copy\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43d247b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pow_reducer(x):\n",
    "  return x.pow(3).sum()\n",
    "\n",
    "\n",
    "def pow_adder_reducer(x, y):\n",
    "  return (2 * x.pow(4) + 3 * y.pow(3) + x.pow(3)*y.pow(5)).sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a564de3d",
   "metadata": {},
   "source": [
    "The inputs are what the Hessian is computed with respect to. Here it is just x's; for us it will be model parameters. This is fine, because loss functions are set up that way: inputs are fixed and the gradients are with respect to params. I am a little unclear on how to pass params to vhp. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "426dbc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ishape = (4)\n",
    "inputs = torch.rand(ishape)\n",
    "v = torch.ones(ishape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ffc7a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.7065), tensor([1.6322, 5.6431, 5.6715, 1.2844]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vhp(pow_reducer, inputs, v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4575b5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7065)\n",
      "tensor([1.6322, 5.6431, 5.6715, 1.2844])\n"
     ]
    }
   ],
   "source": [
    "print(pow_reducer(inputs))\n",
    "print(6*inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70ee8ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.7065, grad_fn=<SumBackward0>),\n",
       " tensor([1.6322, 5.6431, 5.6715, 1.2844], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vhp(pow_reducer, inputs, v, create_graph=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e75f4fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pow_adder_reducer: tensor(4.2575, grad_fn=<SumBackward0>)\n",
      "check pow_adder_reducer: tensor(4.2575) \n",
      "\n",
      "vhp: (tensor([32.3443,  0.7036,  4.0947], grad_fn=<AddBackward0>), tensor([35.9528,  5.5127,  0.5465], grad_fn=<AddBackward0>))\n",
      "[tensor([32.3443, 35.9528]), tensor([0.7036, 5.5127]), tensor([4.0947, 0.5465])] \n",
      "\n",
      "tensor(0., grad_fn=<SubBackward0>) tensor(0., grad_fn=<SubBackward0>)\n",
      "tensor(0., grad_fn=<SubBackward0>) tensor(0., grad_fn=<SubBackward0>)\n",
      "tensor(0., grad_fn=<SubBackward0>) tensor(0., grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "nv = 3\n",
    "inputs = (torch.rand(nv), torch.rand(nv)) #  need an x and a y\n",
    "v = (torch.ones(nv), torch.ones(nv))\n",
    "lp, vH = vhp(pow_adder_reducer, inputs, v, create_graph=True)\n",
    "print('pow_adder_reducer:', lp)\n",
    "print('check pow_adder_reducer:', pow_adder_reducer(*inputs),'\\n')\n",
    "print('vhp:', vH)\n",
    "\n",
    "x, y = inputs\n",
    "H = []\n",
    "vmat = torch.zeros(2, nv)\n",
    "for i in range(nv):\n",
    "    vmat[0,i] = v[0][i]\n",
    "    vmat[1,i] = v[1][i]\n",
    "    H.append(torch.tensor([[(24*x[i]**2 + 6*x[i]*y[i]**5), (15*x[i]**2*y[i]**4)],\\\n",
    "                           [(15*x[i]**2*y[i]**4), (18*y[i]+20*x[i]**3*y[i]**3)]]))\n",
    "\n",
    "vH_check = []\n",
    "for i in range(nv):\n",
    "    vH_check.append(vmat[:,i].t() @ H[i])\n",
    "\n",
    "print(vH_check,'\\n')\n",
    "\n",
    "for i in range(nv):\n",
    "    print(vH[0][i]-vH_check[i][0], vH[1][i]-vH_check[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0146cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.92 27.36\n",
      "(tensor([0.9320, 0.1704, 0.4131]), tensor([0.8937, 0.3059, 0.0304]))\n",
      "[0.70578216 1.7956981  1.17623904]\n"
     ]
    }
   ],
   "source": [
    "print(4*1.23, 6*4.56)\n",
    "print(inputs)\n",
    "print(np.array([19.3102, 49.1303, 32.1819])/(6*4.56))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab75e2f",
   "metadata": {},
   "source": [
    "# VHP with respect to model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74e92c0",
   "metadata": {},
   "source": [
    "The following is from a stackoverflow question about applying vhp when models with parameters are involved, as opposed to simpler functions where we don't have to iterate over parameters.\n",
    "\n",
    "https://stackoverflow.com/questions/68492748/trouble-with-minimal-hvp-on-pytorch-model\n",
    "\n",
    "This in turn linked to\n",
    "\n",
    "https://discuss.pytorch.org/t/hvp-w-r-t-model-parameters/83520\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04eb3cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_gradients(model):\n",
    "    g = {}\n",
    "    for k, v in model.named_parameters():\n",
    "        gnext = v.grad\n",
    "        if gnext is not None:\n",
    "            next_entry = {k: gnext.clone().detach()}\n",
    "        else:\n",
    "            next_entry = {k: None}\n",
    "        g.update(next_entry)\n",
    "    return g\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "187ce865",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# your loss function\n",
    "def objective(X):\n",
    "    return torch.sum(0.25 * torch.sum(X)**4)\n",
    "\n",
    "# Following are utilities to make nn.Module \"functional\", in the sense of \n",
    "#    being from or compatible with the torch.autograd.functional library. \n",
    "#\n",
    "# borrowed from the link I posted in comment\n",
    "def del_attr(obj, names): # why, why, why? But it definitely breaks without this. \n",
    "    if len(names) == 1:\n",
    "        delattr(obj, names[0])\n",
    "    else:\n",
    "        del_attr(getattr(obj, names[0]), names[1:])\n",
    "\n",
    "def set_attr(obj, names, val):\n",
    "    if len(names) == 1:\n",
    "        setattr(obj, names[0], val)\n",
    "    else:\n",
    "        set_attr(getattr(obj, names[0]), names[1:], val)\n",
    "\n",
    "def make_functional(model):\n",
    "    orig_params = tuple(model.parameters())\n",
    "    orig_grad = capture_gradients(model)\n",
    "    # Remove all the parameters in the model, because reasons. \n",
    "    names = []\n",
    "    for name, p in list(model.named_parameters()):\n",
    "        del_attr(model, name.split(\".\"))\n",
    "        names.append(name)\n",
    "    return orig_params, orig_grad, names\n",
    "\n",
    "def restore_model(model, names, params, grad):\n",
    "    load_weights(model, names, params, as_params=True)\n",
    "    for k, v in model.named_parameters():\n",
    "        print('grad is type', type(grad))\n",
    "        print('grad is', grad)\n",
    "        if grad[k]:\n",
    "            v.grad = grad[k].clone().detach()\n",
    "        else:\n",
    "            v.grad = None\n",
    "                \n",
    "\n",
    "\n",
    "def load_weights(model, names, params, as_params=False):\n",
    "    for name, p in zip(names, params):\n",
    "        if not as_params:\n",
    "            set_attr(model, name.split(\".\"), p)\n",
    "        else:\n",
    "            set_attr(model, name.split(\".\"), torch.nn.Parameter(p))\n",
    " \n",
    "        \n",
    "\n",
    "\n",
    "# This is how we trick vhp into doing the Hessian with respect to params and not other inputs. \n",
    "def loss_wrt_params(*new_params):\n",
    "    load_weights(mlp, names, new_params) # Weird! We removed the params before. \n",
    "\n",
    "    x = torch.ones((Arows,))\n",
    "    out = mlp(x)\n",
    "    loss = objective(out)\n",
    "    loss.backward(retain_graph=True)\n",
    "    return loss\n",
    "\n",
    "# your simple MLP model\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Forward pass'''\n",
    "        return self.layers(x)\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e47a83ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your model instantiation\n",
    "Arows = 2\n",
    "Acols = 2\n",
    "mlp = SimpleMLP(Arows, Acols)\n",
    "\n",
    "v_to_dot = tuple([p.clone().detach() for p in mlp.parameters()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ceb7b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.weight : Parameter containing:\n",
      "tensor([[ 0.0313, -0.1776],\n",
      "        [ 0.3404,  0.3687]], requires_grad=True)\n",
      "layers.0.bias : Parameter containing:\n",
      "tensor([ 0.2553, -0.6846], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for n, p in mlp.named_parameters():\n",
    "    print(n,':',p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5802a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make model's parameters functional\n",
    "orig_params, orig_grad, names = make_functional(mlp)\n",
    "params2pass = tuple(p.detach().requires_grad_() for p in orig_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89ab1ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad is type <class 'dict'>\n",
      "grad is {'layers.0.weight': None, 'layers.0.bias': None}\n",
      "grad is type <class 'dict'>\n",
      "grad is {'layers.0.weight': None, 'layers.0.bias': None}\n",
      "loss: tensor(7.9300e-05)\n",
      "H(params): (tensor([[0.0071, 0.0071],\n",
      "        [0.0071, 0.0071]]), tensor([0.0071, 0.0071]))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss_value, hessian = torch.autograd.functional.vhp(loss_wrt_params, params2pass, \\\n",
    "                                                    v_to_dot, strict=True)\n",
    "\n",
    "restore_model(mlp, names, orig_params, orig_grad)\n",
    "loss_wrt_params(*orig_params) # this calls backward on loss = objective(out)\n",
    "grad = capture_gradients(mlp)\n",
    "\n",
    "print('loss:', loss_value)\n",
    "print('H(params):', hessian)\n",
    "\n",
    "#L=torch.sum(0.25 * torch.sum(Y)**4)\n",
    "#Y = W * x + B\n",
    "# dLdY = sum(Y)\n",
    "# dYdWij = sum(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "713865f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layers.0.weight': tensor([[0.0024, 0.0024],\n",
       "         [0.0024, 0.0024]]),\n",
       " 'layers.0.bias': tensor([0.0024, 0.0024])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capture_gradients(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "959d3630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layers.0.weight': tensor([[0.0024, 0.0024],\n",
       "         [0.0024, 0.0024]]),\n",
       " 'layers.0.bias': tensor([0.0024, 0.0024])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c797e82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# objective(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0380b83",
   "metadata": {},
   "source": [
    "# Argh, what is a leaf node? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b475aaa",
   "metadata": {},
   "source": [
    "The following is meant to be an example of what leaf nodes are. I think it might be helpful, even though the blog post it came from is super hard to understand and explains itself poorly. http://www.bnikolic.co.uk/blog/pytorch/python/2021/03/15/pytorch-leaf.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1f63550",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "x=torch.ones(10, requires_grad=True)\n",
    "y=torch.ones(10, requires_grad=True)\n",
    "\n",
    "# The remaining nodes are not leaves:\n",
    "def H(z1, z2):\n",
    "    ret = torch.sin(z1**3)*torch.cos(z2**2)\n",
    "    return ret\n",
    "# dHdz1 = cos(z1**3)*cos(z2**2)*3*z1**2\n",
    "# dHdz2 = -sin(z1**3)*sin(z2**2)*2*z2\n",
    "\n",
    "def G(z1, z2):\n",
    "    return torch.exp(z1)+torch.log(z2)\n",
    "\n",
    "def F(z1, z2):\n",
    "    return z1**3*z2**0.5\n",
    "\n",
    "h=H(x,y)\n",
    "\n",
    "g=G(x,y)\n",
    "\n",
    "f=F(h,g)\n",
    "f.retain_grad()\n",
    "f.sum().backward() # must sum to get a scalar, otherwise backward() will barf. \n",
    "                    #  No gradients are loaded until backawrd() is called. \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12fbde4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hchk = np.sin(1)*np.cos(1)\n",
    "gchk = np.exp(1) + np.log(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c994a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "dFdh = 3*hchk**2 *np.sqrt(gchk)\n",
    "dFdg = 0.5* hchk**3 / np.sqrt(gchk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d54b4bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dgdx = np.exp(1)\n",
    "dgdy = 1.0\n",
    "dhdx = np.cos(1)*np.cos(1)*3\n",
    "dhdy = -np.sin(1)*np.sin(1)*2\n",
    "#-sin(z1**3)*sin(z2**2)*2*z2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c014567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dFdx = 0.9728684287087229\n",
      "dFdy = -1.419366770452894\n"
     ]
    }
   ],
   "source": [
    "dFdx = dFdh*dhdx + dFdg*dgdx\n",
    "dFdy = dFdh*dhdy + dFdg*dgdy\n",
    "print('dFdx =', dFdx)\n",
    "print('dFdy =', dFdy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5cdf7cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.1921e-07, -1.1921e-07, -1.1921e-07, -1.1921e-07, -1.1921e-07,\n",
      "        -1.1921e-07, -1.1921e-07, -1.1921e-07, -1.1921e-07, -1.1921e-07])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(dFdx - x.grad)\n",
    "print(dFdy - y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a8e4368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9a99eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d505b8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 3*a**3 - b**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19e33e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "588a8ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b05f8d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([36., 81.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "237d92cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-12.,  -8.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
